# Отчёт о результатах эксперимента


### Описание эксперимента

Был реализован supernet слой, содержащий несколько разных сверточных блоков, только один из которых в конкретный момент времени является активным. Была реализована модель, состоящая из двух supernet слоёв, в каждом из которых находяться по два свёрточных слоя с различными размероми ядра свёртки, а также "головы" состоящей из двух полносвязных слоёв. После каждого из свёрточных слоёв добавлена нелинейная функция активации (ReLU) а также операция MaxPooling. На вход последних двух слоёв подаётся эмбеддинг фиксированного размера, в случае если на выходе из fully-convolutional части модели получается меньшее количество параметров - часть эмбеддинга заполняется нулями. Также были реализованы методы, выделить подмножество весов, составляющее один из возможных путей выполнения в отдельную, полностью функциональную модель.

Далее, было обучено 5 моделей:
* Модель в которой для каждого батча данных свёрточные блоки выбирались случайно и с равной вероятностью из четырёх возможных конфигураций (все остальные веса были общими, и обновлялись на каждой итерации)
* Четыре модели с фиксированной архитектурой, соответсвующие каждому из возможных путей выполнения

Модель училась на датасете MNIST, единственным шагом препроцессинга была нормализация датасета. Оптимизация производилось с помощью алгоритма Adam, с фиксированным LR. Все гиперпараметры были подобраны поверхностным случайным поиском, и были одинаковы для всех моделей

### Результаты эксперимента

| Experiment | Configuration | Accuracy | Epoch |
|------------|---------------|----------|-------|
| Standalone | 3x3->3x3      |  .9885   |       |
| Standalone | 3x3->3x3      |  .9894   | 287   |
| Standalone | 3x3->3x3      |  .9885   | 256   |
| Standalone | 3x3->3x3      |  .9885   | 250   |
|------------|---------------|----------|-------|
| Joint      | 3x3->3x3      |          |       |
| Joint      | 3x3->3x3      |          |       |
| Joint      | 3x3->3x3      |          |       |
| Joint      | 3x3->3x3      |          |       |
|------------|---------------|----------|-------|


При совместном обучении четырёх моделей с общей головой итоговое качество каждой из них оказалась практически равна точности соответсвующей модели обучаемой отдельно. Возможно, какая-то разница в качестве моделей и есть, но для того, чтобы её выявить эксперимент необходимо будет повторить некоторое количество раз, до достижения статистической значимости. В целом, точность классификации на отложенной выборке для всех моделей после обучения находится в инетрвале [0.989, 0.99] и практически не отличается у разных архитектур.

![Training curves](pics/training_curves.png "Training curves")

С точки зрения времени обучения, совместная модель сходилась дольше, чем любая из отдельных, но существенным образом быстрее, чем если бы мы обучали все четыре модели последовательно. Интересно, что на кривых обучения есть несколько ярко выраженных "ступенек". Можно предположить, что это связанно с тем, что отдельные подмножества эмбеддинга, подающегося на вход последних слоёв не являются информативными для части батчей, и модель игнорирует их, до тех пор, пока не израсходованная ёмкость оставшегося подмножества модели, но эта гипотеза требует подтверждения дальнейшими эксперииментами.

Также, как и ожидалось, при совместном обучнении максимальная точность на отложенной выборке для каждой из подмоделей достигается в разные моменты. Это неминуемо приводит к тому, что большая часть моделей будет либо переобучена, либо недообучена. Можно предположить, что с помошью адаптивного LR, неравномерного сэмплинга подмоделей при обучени или "замораживания" части весов от этого эффекта возможно избавиться.

### Дальнейшие направления для исследований

Во многих архитектурах one-shot networks используется схема, в которой одновременно обучаются несколько параллельных блоков. Во многих случаях этого можно достичь просто агрегируя их выходы, в приложенной статье предлагалось использовать 1x1 свёртки, чтобы количество каналов на выходе оставалось постоянным.

Одной очевидной проблемой текущего подхода является неодновременное достижение оптимального состояния у разных подмоделей. Есть несколько подходов, которые можно попробовать для решения этой проблемы - например разный LR для разных блоков или неравномерное сэмплирование подмоделей, пропорционально текущим метрикам.

Для чистоты эксперимента, и с учётом того, что мы работаем с небольшим датасетом хотелось бы повторить процесс обучения каждой из моделей N раз, для того, чтобы получить оценки вариабельности метрик. Также, в идеале, перед обучением хотелось бы провести процесс подбора гиперпараметров для каждой из моделей

Также, по субъективному ощущению, совместное обучение моделей очень чувствительно к batch_size ( из-за того, что в моей реализации подмодель выбирается для всего батча ). Возможно, маскирование модели для каждого примера в батче положительно скажется на сходимости.
