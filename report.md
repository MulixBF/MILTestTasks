# Отчёт о результатах эксперимента


### Описание эксперимента

Был реализован supernet слой, содержащий несколько разных сверточных блоков, только один из которых в конкретный момент времени является активным. Была реализована модель, состоящая из двух supernet слоёв, в каждом из которых находяться по два свёрточных слоя с различными размероми ядра свёртки, а также "головы" состоящей из двух полносвязных слоёв. После каждого из свёрточных слоёв добавлена нелинейная функция активации (ReLU) а также операция MaxPooling. На вход последних двух слоёв подаётся эмбеддинг фиксированного размера, в случае если на выходе из fully-convolutional части модели получается меньшее количество параметров - часть эмбеддинга заполняется нулями. Также были реализованы методы, позволяющие выделить подмножество весов, составляющее один из возможных путей выполнения в отдельную, полностью функциональную модель.

Далее, было обучено 5 моделей:
* Модель в которой для каждого батча данных свёрточные блоки выбирались случайно и с равной вероятностью из четырёх возможных конфигураций (все остальные веса были общими, и обновлялись на каждой итерации)
* Четыре модели с фиксированной архитектурой, соответсвующие каждому из возможных путей выполнения

Модель училась на датасете MNIST, единственным шагом препроцессинга была нормализация изображения. Оптимизация производилось с помощью алгоритма Adam, с фиксированным LR. Все гиперпараметры были грубо подобраны случайным поиском, и одинаковы для всех моделей

### Результаты эксперимента

| Experiment | Configuration | Accuracy | Epoch |
|------------|---------------|----------|-------|
| Standalone | 3x3->3x3      |  .9885   |       |
| Standalone | 3x3->3x3      |  .9894   | 287   |
| Standalone | 3x3->3x3      |  .9885   | 256   |
| Standalone | 3x3->3x3      |  .9885   | 250   |
|------------|---------------|----------|-------|
| Joint      | 3x3->3x3      |          |       |
| Joint      | 3x3->3x3      |          |       |
| Joint      | 3x3->3x3      |          |       |
| Joint      | 3x3->3x3      |          |       |
|------------|---------------|----------|-------|


При совместном обучении четырёх моделей с общей головой итоговая точность классификации на валидационной выборке для каждой из них оказалась практически равна точности соответсвующей модели обучаемой отдельно. В целом, точность классификации на отложенной выборке для всех моделей после обучения находится в инетрвале [0.989, 0.99] и практически не отличается у разных архитектур.

![Training curves](pics/training_curves.png "Training curves")

С точки зрения времени обучения, совместная модель сходилась дольше, чем любая из отдельных, но существенным образом быстрее, чем если бы мы обучали все четыре модели последовательно. Интересно, что на кривых обучения совместно обучаемой модели есть несколько ярко выраженных "ступенек". Можно предположить, что это связанно с тем, что отдельные подмножества эмбеддинга, подающегося на вход последних слоёв не являются информативными для части батчей, и модель игнорирует их, до тех пор, пока не израсходованная ёмкость оставшегося подмножества модели, но эта гипотеза требует подтверждения дальнейшими экспериментами.

Также, как и ожидалось, максимальная точность на отложенной выборке при совместном обучени достигается для каждой из подмоделей не одновременно. Это приводит к тому, что большая часть моделей будет либо переобучена, либо недообучена и здесь есть пространство для улучшения архитектуры.

### Дальнейшие направления для исследований

Во многих архитектурах one-shot networks используется схема, в которой одновременно обучаются несколько параллельных блоков. Во многих случаях этого можно достичь просто агрегируя их выходы, в приложенной статье предлагалось использовать 1x1 свёртки, чтобы количество каналов на выходе оставалось постоянным.

Одной очевидной проблемой текущего подхода является неодновременное достижение оптимального состояния у разных подмоделей. Есть несколько подходов, которые можно попробовать для решения этой проблемы - например разный LR для разных блоков или неравномерное сэмплирование подмоделей, пропорционально текущим метрикам.

Для чистоты эксперимента, и с учётом того, что мы работаем с небольшим датасетом хотелось бы повторить процесс обучения каждой из моделей N раз, для того, чтобы получить оценки вариабельности метрик. Также, в идеале, перед обучением хотелось бы провести процесс подбора гиперпараметров для каждой из моделей

Также, по субъективному ощущению, совместное обучение моделей очень чувствительно к batch_size ( из-за того, что в моей реализации подмодель выбирается для всего батча ). Возможно, маскирование модели для каждого примера в батче положительно скажется на сходимости.
