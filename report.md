# Отчёт о результатах эксперимента


### Описание эксперимента

Был реализован supernet слой, содержащий несколько разных сверточных блоков, только один из которых в конкретный момент времени является активным. Была реализована модель, состоящая из двух supernet слоёв, в каждом из которых находяться по два свёрточных слоя с различными размероми ядра свёртки, а также "головы" состоящей из двух полносвязных слоёв. После каждого из свёрточных слоёв добавлена нелинейная функция активации (ReLU) а также операция MaxPooling. На вход последних двух слоёв подаётся эмбеддинг фиксированного размера, в случае если на выходе из fully-convolutional части модели получается меньшее количество параметров - часть эмбеддинга заполняется нулями.

Далее, было обучено 5 моделей:
* Модель в которой для каждого батча данных свёрточные блоки выбирались случайно и с равной вероятностью из четырёх возможных конфигураций (все остальные веса были общими, и обновлялись на каждой итерации)
* Четыре модели с фиксированной архитектурой, соответсвующие каждому из возможных путей выполнения

Модель училась на датасете MNIST, единственным шагом препроцессинга была нормализация изображения. Оптимизация производилось с помощью алгоритма Adam, с фиксированным LR. Все гиперпараметры были грубо подобраны случайным поиском, и одинаковы для всех моделей

### Результаты эксперимента

| Experiment | Configuration | Accuracy | Epoch |
|------------|---------------|----------|-------|
| Standalone | 3x3->3x3      |  .9885   |  300  |
| Standalone | 3x3->5x5      |  .9894   |  287  |
| Standalone | 5x5->3x3      |  .9893   |  256  |
| Standalone | 5x5->5x5      |  .9896   |  250  |
| Joint      | 3x3->3x3      |  .9877   |  342  |
| Joint      | 3x3->5x5      |  .9881   |  348  |
| Joint      | 5x5->3x3      |  .9890   |  350  |
| Joint      | 5x5->5x5      |  .9887   |  344  |


При совместном обучении четырёх моделей с общей головой итоговая точность классификации близка к точности соответсвующих моделей обучаемых отдельно. В целом, точность классификации у разных архитектур кардинально не отличается. К сожалению, у меня не было времени, для того чтобы прогнать обучение несколько раз и оценить чувствительность моделей к параметрам инициализации, но я подозреваю, что разница между ними будет в пределах погрешности.

![Training curves](https://raw.githubusercontent.com/MulixBF/MILTestTasks/task/NAS-ImageNet/pics/trainig_curves.png "Training curves")

С точки зрения времени обучения, совместная модель сходилась дольше, чем любая из обучаемых отдельно, но существенным образом быстрее, чем если бы мы обучали все четыре модели последовательно. Интересно, что на кривых обучения у совместно обучаемой модели есть несколько ярко выраженных "ступенек". Можно предположить, что это связанно с тем, что отдельные подмножества эмбеддинга, подающегося на вход последних слоёв не являются информативными для части батчей, и модель игнорирует их, до тех пор, пока полностью не обучится использовать оставшуюся часть, но эта гипотеза требует подтверждения дальнейшими экспериментами.

Также, как и ожидалось, максимальная точность на отложенной выборке при совместном обучени достигается для каждой из подмоделей не одновременно. Это приводит к тому, что большая часть моделей будет либо переобучена, либо недообучена, и здесь есть пространство для улучшения архитектуры.

### Дальнейшие направления для исследований

Во многих архитектурах one-shot networks используется схема, в которой одновременно обучаются несколько параллельных блоков. Во многих случаях этого можно достичь просто агрегируя их выходы, в приложенной статье предлагалось использовать 1x1 свёртки, чтобы количество каналов на выходе оставалось постоянным.

Одной очевидной проблемой текущего подхода является неодновременное достижение оптимального состояния у разных подмоделей. Есть несколько подходов, которые можно попробовать для решения этой проблемы - например разный LR для разных блоков или неравномерное сэмплирование подмоделей, пропорционально текущим метрикам.

Для чистоты эксперимента, и с учётом того, что мы работаем с небольшим датасетом хотелось бы повторить процесс обучения каждой из моделей N раз, для того, чтобы получить оценки вариабельности метрик. Также, в идеале, перед обучением хотелось бы провести процесс подбора гиперпараметров для каждой из моделей

Также, эксперименты показали, что совместное обучение моделей очень чувствительно к batch_size из-за того, что в моей реализации подмодель выбирается для всего батча. Возможно, выбор подмодели на уровне отдельных примеров мог бы быть полезным
